{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, LeakyReLU, Dense, Dropout, MaxPool1D, GlobalMaxPool1D\n",
    "\n",
    "\n",
    "class ResBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filter):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.filter = filter\n",
    "        self.pooling_window = 3\n",
    "\n",
    "        self.conv1a = tf.keras.layers.Conv1D(filter, kernel_size, padding='same')\n",
    "        self.bn1a = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1b = tf.keras.layers.Conv1D(filter, kernel_size, padding='same')\n",
    "        self.bn1b = tf.keras.layers.BatchNormalization()\n",
    "        self.pool = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        # Convolution layers\n",
    "        x = self.conv1a(input_tensor)\n",
    "        x = self.bn1a(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = self.bn1b(x, training=training)\n",
    "        # Combine input and output into residual information\n",
    "        if self.filter == 1:\n",
    "            x += input_tensor\n",
    "        else:\n",
    "            x = tf.concat([x, input_tensor], 2)\n",
    "        # Pooling\n",
    "        x = self.pool(x)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "class RCNNmodel(tf.keras.Model):\n",
    "    def __init__(self, specs):\n",
    "        n_res, kernel_size, filters, n_ffl, n_classes = specs\n",
    "        super(RCNNmodel, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.n_res = n_res\n",
    "        self.blocks = tf.keras.Sequential()\n",
    "        for _ in range(n_res):\n",
    "            self.blocks.add(ResBlock(kernel_size=kernel_size, filter=filters[_]))\n",
    "        self.blocks.add(GlobalMaxPool1D())\n",
    "        self.ffl_block = tf.keras.Sequential()\n",
    "        self.ffl_block._name = 'ffl_block'\n",
    "        for _ in range(n_ffl):\n",
    "            self.ffl_block.add(Dense(1024))\n",
    "            self.ffl_block.add(Dropout(0.1))\n",
    "            self.ffl_block.add(BatchNormalization())\n",
    "            self.ffl_block.add(LeakyReLU())\n",
    "        self.output_layer = Dense(n_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.ffl_block(x)\n",
    "        x = self.output_layer(x)\n",
    "        if self.n_classes == 1:\n",
    "            return tf.nn.sigmoid(x)\n",
    "        else:\n",
    "            return tf.nn.softmax(x)\n",
    "\n",
    "\n",
    "class CNNmodel(tf.keras.Model):\n",
    "    def __init__(self, specs):\n",
    "        super(CNNmodel, self).__init__()\n",
    "        n_cnn, kernel_sizes, filters, n_classes = specs\n",
    "        self.model = tf.keras.Sequential()\n",
    "        for _ in range(n_cnn):\n",
    "            self.model.add(Conv1D(filters[_], kernel_size=kernel_sizes[_], activation=tf.keras.activations.relu,\n",
    "                                  padding='valid'))\n",
    "            self.model.add(Conv1D(filters[_], kernel_size=kernel_sizes[_], activation=tf.keras.activations.relu,\n",
    "                                  padding='valid'))\n",
    "            if _ < (n_cnn - 1):\n",
    "                self.model.add(MaxPool1D(pool_size=2))\n",
    "                self.model.add(Dropout(0.1))\n",
    "            else:\n",
    "                self.model.add(GlobalMaxPool1D())\n",
    "                self.model.add(Dropout(0.2))\n",
    "        self.ffl_block = tf.keras.Sequential()\n",
    "        self.ffl_block._name = 'ffl_block'\n",
    "        self.ffl_block.add(Dense(64, activation=tf.keras.activations.relu, name=\"dense_1\"))\n",
    "        self.ffl_block.add(Dense(64, activation=tf.keras.activations.relu, name=\"dense_2\"))\n",
    "        self.ffl_block.add(Dense(n_classes, activation=tf.keras.activations.sigmoid, name=\"dense_3_ptbdb\"))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.model(x)\n",
    "        return self.ffl_block(x)\n",
    "\n",
    "\n",
    "class RNNmodel(tf.keras.Model):\n",
    "    def __init__(self, specs):\n",
    "        super(RNNmodel, self).__init__()\n",
    "        n_rnn, use_cnn, cnn_window, cnn_emb_size, hidden_size, type, n_ffl, n_classes = specs\n",
    "        self.n_classes = n_classes\n",
    "        self.use_cnn = use_cnn\n",
    "        self.cnn_1x1 = tf.keras.layers.Conv1D(cnn_emb_size, cnn_window, padding='same')\n",
    "        self.rnn_block = tf.keras.Sequential()\n",
    "        if type is 'bidir':\n",
    "            self.rnn_blocks = layers.Bidirectional(layers.LSTM(hidden_size, activation=tf.keras.activations.sigmoid, return_sequences=True))\n",
    "            self.rnn_out = layers.Bidirectional(layers.LSTM(hidden_size, activation=tf.keras.activations.sigmoid))\n",
    "        elif type is 'LSTM':\n",
    "            self.rnn_blocks = layers.LSTM(hidden_size, return_sequences=True, activation=tf.keras.activations.sigmoid)\n",
    "            self.rnn_out = layers.LSTM(hidden_size, activation=tf.keras.activations.sigmoid)\n",
    "        elif type is 'GRU':\n",
    "            self.rnn_blocks = layers.GRU(hidden_size, activation=tf.keras.activations.sigmoid, return_sequences=True)\n",
    "            self.rnn_out = layers.GRU(hidden_size, activation=tf.keras.activations.sigmoid)\n",
    "        else:\n",
    "            print(\"'type' has to be 'bidir', 'LSTM' or 'GRU'.\")\n",
    "        for _ in range(n_rnn - 1):\n",
    "            self.rnn_block.add(self.rnn_blocks)\n",
    "        self.rnn_block.add(self.rnn_out)\n",
    "        self.ffl_block = tf.keras.Sequential()\n",
    "        self.ffl_block._name = 'ffl_block'\n",
    "        for _ in range(n_ffl):\n",
    "            self.ffl_block.add(tf.keras.layers.Dense(1024))\n",
    "            self.ffl_block.add(tf.keras.layers.Dropout(0.1))\n",
    "            self.ffl_block.add(tf.keras.layers.BatchNormalization())\n",
    "            self.ffl_block.add(tf.keras.layers.LeakyReLU())\n",
    "        self.output_layer = tf.keras.layers.Dense(n_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.use_cnn:\n",
    "            x = self.cnn_1x1(x)\n",
    "        x = self.rnn_block(x)\n",
    "        x = self.ffl_block(x)\n",
    "        x = self.output_layer(x)\n",
    "        if self.n_classes == 1:\n",
    "            return tf.nn.sigmoid(x)\n",
    "        else:\n",
    "            return tf.nn.softmax(x)\n",
    "\n",
    "\n",
    "class Ensemble_FFL_block(tf.keras.Model):\n",
    "    def __init__(self, specs):\n",
    "        super(Ensemble_FFL_block, self).__init__()\n",
    "        n_ffl, dense_layer_size, n_classes = specs\n",
    "        self.n_classes = n_classes\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model._name = 'ffl_block'\n",
    "        for _ in range(n_ffl):\n",
    "            self.model.add(tf.keras.layers.Dense(dense_layer_size))\n",
    "            self.model.add(tf.keras.layers.Dropout(0.1))\n",
    "            self.model.add(tf.keras.layers.BatchNormalization())\n",
    "            self.model.add(tf.keras.layers.LeakyReLU())\n",
    "        self.output_layer = tf.keras.layers.Dense(n_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.output_layer(x)\n",
    "        if self.n_classes == 1:\n",
    "            return tf.nn.sigmoid(x)\n",
    "        else:\n",
    "            return tf.nn.softmax(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/'\n",
    "DATA_PATH = 'data/'\n",
    "\n",
    "CNN_SPECS = (4, [5, 3, 3, 3], [16, 32, 32, 256], 1)\n",
    "RCNN_SPECS = (4, 3, [3, 12, 48, 192], 2, 1)\n",
    "RNN_SPECS = (2, False, 3, 16, 256, 'LSTM', 2, 1)\n",
    "ENSEMBLE_SPECS = (2, 1024, 1)\n",
    "SPEC_LIST = {'cnn': CNN_SPECS,\n",
    "             'rcnn': RCNN_SPECS,\n",
    "             'rnn': RNN_SPECS,\n",
    "             'ensemble': ENSEMBLE_SPECS}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def architect(mode, data, type, run_id, type_ids=None):\n",
    "    if isinstance(data, str):\n",
    "        data = [data]\n",
    "    if isinstance(type, str):\n",
    "        type = [type]\n",
    "    id = run_id\n",
    "    # Testing\n",
    "    if mode is 'training':\n",
    "        optimizers = ['Adam']\n",
    "        # dropouts = [0.1, 0.5]\n",
    "        # n_layers = [1, 2, 3]\n",
    "        lr_list = [0.01, 0.001]\n",
    "        for d in data:\n",
    "            for t in type:\n",
    "                for o in optimizers:\n",
    "                    for lr in lr_list:\n",
    "                        if o is 'Adam':\n",
    "                            opt = tf.keras.optimizers.Adam(lr)\n",
    "                        specs = SPEC_LIST[t]\n",
    "                        if d is 'mitbih':\n",
    "                            specs = list(specs)\n",
    "                            specs[-1] = 5\n",
    "                            specs = tuple(specs)\n",
    "\n",
    "                        m = get_architecture(t, specs)\n",
    "                        training(m, opt, d, t, id)\n",
    "    # Testing\n",
    "    if mode is 'testing':\n",
    "        for d in data:\n",
    "            for t in type:\n",
    "                specs = SPEC_LIST[t]\n",
    "                if d is 'mitbih':\n",
    "                    specs = list(specs)\n",
    "                    specs[-1] = 5\n",
    "                    specs = tuple(specs)\n",
    "                m = get_architecture(t, specs)\n",
    "                testing(m, d, t, id)\n",
    "    if mode is 'ensemble':\n",
    "        run_ensemble(data=data, type_ids=type_ids, id=run_id)\n",
    "    if mode is 'visualization':\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def training(model, opt, data, type, id):\n",
    "    file_path = MODEL_PATH + type + '_' + data + '_' + str(id) + '.h5'\n",
    "    if type is 'tfl':\n",
    "        save = False\n",
    "        print(\"Not saving best models... not implemented for submodules!\")\n",
    "    else:\n",
    "        save = True\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=save, mode='max')\n",
    "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "    redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "    callbacks_list = [checkpoint, early, redonplat]\n",
    "    if data is 'mitbih':\n",
    "        Y, X, _, _ = get_mitbih()\n",
    "        model.compile(optimizer=opt, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    else:\n",
    "        Y, X, _, _ = get_ptbdb()\n",
    "        model.compile(optimizer=opt, loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])\n",
    "    if save:\n",
    "        model.fit(X, Y, epochs=1, callbacks=callbacks_list, validation_split=0.1)\n",
    "    else:\n",
    "        # NO CHECKPOINTS FOR TFL -> due to using submodules the save-implementation broke\n",
    "        model.fit(X, Y, callbacks=[early, redonplat], validation_split=0.1)\n",
    "        model.save_weights(filepath=file_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def testing(model, data, type, id):\n",
    "    file_path = MODEL_PATH + type + '_' + data + '_' + str(id) + '.h5'\n",
    "    print(file_path)\n",
    "    if data is 'mitbih':\n",
    "        _, _, Y_test, X_test = get_mitbih()\n",
    "    else:\n",
    "        _, _, Y_test, X_test = get_ptbdb()\n",
    "    model.build(input_shape=(None, X_test.shape[1], X_test.shape[2]))\n",
    "    model.load_weights(file_path)\n",
    "    pred_test = model.predict(X_test)\n",
    "    pred_test = np.argmax(pred_test, axis=-1)\n",
    "    f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
    "    print(\"Test f1 score : %s \" % f1)\n",
    "    acc = accuracy_score(Y_test, pred_test)\n",
    "    print(\"Test accuracy score : %s \" % acc)\n",
    "    return {'target': Y_test, 'prediction': pred_test}\n",
    "\n",
    "\n",
    "def get_architecture(type, specs):\n",
    "    if type is 'cnn':\n",
    "        return CNNmodel(specs)\n",
    "    elif type is 'rcnn':\n",
    "        return RCNNmodel(specs)\n",
    "    elif type is 'rnn':\n",
    "        return RNNmodel(specs)\n",
    "    elif type is 'ensemble':\n",
    "        return Ensemble_FFL_block(specs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_models(data, type_ids):\n",
    "    print(os.getcwd())\n",
    "    if isinstance(data, list):\n",
    "        data = data[0]\n",
    "    if isinstance(type_ids, tuple):\n",
    "        type_ids = [type_ids]\n",
    "    model_list = []\n",
    "    for ti in type_ids:\n",
    "        t = ti[0]\n",
    "        id = ti[1]\n",
    "        file_path = MODEL_PATH + t + '_' + data + '_' + str(id) + '.h5'\n",
    "        print(file_path)\n",
    "        specs = SPEC_LIST[t]\n",
    "        empty = get_architecture(t, specs)\n",
    "        empty.build(input_shape=(None, 187, 1))\n",
    "        empty.load_weights(file_path)\n",
    "        model_list.append(empty)\n",
    "    return model_list\n",
    "\n",
    "\n",
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(models, data):\n",
    "    if data is 'mitbih':\n",
    "        Y, X, Y_test, X_test = get_mitbih()\n",
    "    else:\n",
    "        Y, X, Y_test, X_test = get_ptbdb()\n",
    "    stacked_X = None\n",
    "    stacked_X_test = None\n",
    "    for model in models:\n",
    "        y = model.predict(X, verbose=0)\n",
    "        y_test = model.predict(X_test, verbose=0)\n",
    "        if stacked_X is None:\n",
    "            stacked_X = y\n",
    "            stacked_X_test = y_test\n",
    "        else:\n",
    "            stacked_X = np.dstack((stacked_X, y))\n",
    "            stacked_X_test = np.dstack((stacked_X_test, y_test))\n",
    "    stacked_X = stacked_X.reshape((stacked_X.shape[0], stacked_X.shape[1] * stacked_X.shape[2]))\n",
    "    stacked_X_test = stacked_X_test.reshape((stacked_X_test.shape[0], stacked_X_test.shape[1] * stacked_X_test.shape[2]))\n",
    "    return stacked_X, Y, stacked_X_test, Y_test\n",
    "\n",
    "\n",
    "def load_ensemble_nn(data):\n",
    "    specs = SPEC_LIST['ensemble']\n",
    "    if data is 'mitbih':\n",
    "        specs = list(specs)\n",
    "        specs[-1] = 5\n",
    "        specs = tuple(specs)\n",
    "    return Ensemble_FFL_block(specs)\n",
    "\n",
    "\n",
    "# specify settings\n",
    "def run_ensemble(data, type_ids, id=500, mode='nn'):\n",
    "    # mode can be mean, logistic or nn\n",
    "    # load all corresponding models into model-list\n",
    "    models = load_models(data, type_ids)\n",
    "    # predict datasets with models to generate new ensemble dataset\n",
    "    X, Y, X_test, Y_test = stacked_dataset(models, data)\n",
    "    if mode is 'nn':\n",
    "        file_path = MODEL_PATH + 'ensemble_' + data[0] + '_' + str(id) + '.h5'\n",
    "        model = load_ensemble_nn(data)\n",
    "        opt = tf.keras.optimizers.Adam(0.001)\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "        redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "        callbacks_list = [checkpoint, early, redonplat]\n",
    "        if data is 'mitbih':\n",
    "            model.compile(optimizer=opt, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "        else:\n",
    "            model.compile(optimizer=opt, loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])\n",
    "        model.fit(X, Y, epochs=100, callbacks=callbacks_list, validation_split=0.1)\n",
    "        model.predict(X_test, Y_test)\n",
    "    # Todo: Use a simple mean of the predictions and a logistic regression for comparsion\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def transfer_learning(data_tfl, data, type_id, id=700, freeze=True):\n",
    "    tfl_model = load_models(data_tfl, type_id)[0]\n",
    "    comb_model = tf.keras.Sequential()\n",
    "    for j, layer in enumerate(tfl_model.layers):\n",
    "        if layer.name is 'ffl_block':\n",
    "            del_id = j\n",
    "    for layer in tfl_model.layers[:-del_id]:  # just exclude last layer from copying\n",
    "        comb_model.add(layer)\n",
    "    if freeze:\n",
    "        for layer in comb_model.layers:\n",
    "            layer.trainable = False\n",
    "    ffl_block = load_ensemble_nn(data)\n",
    "    comb_model.add(ffl_block)\n",
    "    opt = tf.keras.optimizers.Adam(0.001)\n",
    "    input_shape = (None, 187, 1)\n",
    "    comb_model.build(input_shape)\n",
    "    trained_model = training(comb_model, opt, data, 'tfl', id)\n",
    "    output = testing(trained_model, data, 'tfl', id)\n",
    "    df = pd.DataFrame.from_dict(output, orient=\"index\")\n",
    "    df.to_csv(\"results_tfl.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "print(\"VISUALIZATION\")\n",
    "n_examples = 5\n",
    "\n",
    "# Individual visualization mitbih\n",
    "vis_data('mitbih', n_examples)\n",
    "\n",
    "# Individual visualization ptbdb\n",
    "vis_data('ptbdb', n_examples)\n",
    "\n",
    "# Global visualization mitbih\n",
    "# Was sötte mer da mache?\n",
    "# Global visualization ptbdb\n",
    "\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Convolutional Networks\n",
    "print(\"CONVOLUTIONAL NETWORKS\")\n",
    "\n",
    "# Run CNN on mitbih\n",
    "# architect(mode='training', data='mitbih', type='cnn', run_id=100)\n",
    "# architect('testing', 'mitbih', 'cnn', 100)\n",
    "\n",
    "# Run CNN on ptbdb\n",
    "# architect(mode='training', data='ptbdb', type='cnn', run_id=150)\n",
    "# architect('testing', 'ptbdb', 'cnn', 150)\n",
    "\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Residual Networks\n",
    "print(\"RESIDUAL CONVOLUTIONAL NETWORKS\")\n",
    "\n",
    "# Run RCNN on mitbih\n",
    "# architect('training', 'mitbih', 'rcnn', 200)\n",
    "# architect('testing', 'mitbih', 'rcnn', 200)\n",
    "\n",
    "# Run RCNN on ptbdb\n",
    "# architect('training', 'ptbdb', 'rcnn', 250)\n",
    "# architect('testing', 'ptbdb', 'rcnn', 250)\n",
    "\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Recurrent Networks\n",
    "print(\"RECURRENT NETWORKS\")\n",
    "\n",
    "# Run RNN on mitbih\n",
    "# architect('training', 'mitbih', 'rnn', 300)\n",
    "# architect('testing', 'mitbih', 'rnn', 300)\n",
    "\n",
    "# Run RNN on ptbdb\n",
    "# architect('training', 'ptbdb', 'rnn', 350)\n",
    "# architect('testing', 'ptbdb', 'rnn', 350)\n",
    "\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Ensemble of Networks\n",
    "print(\"ENSEMBLE NETWORKS\")\n",
    "\n",
    "# define stacked model from multiple member input models\n",
    "# architect('ensemble', 'mitbih', 'ensemble', 500, type_ids = [('rcnn', 200), ('cnn', 100)])\n",
    "# architect('ensemble', 'mitbih', 'ensemble', 500, type_ids = [('rcnn', 200), ('cnn', 100)])\n",
    "\n",
    "# Run RNN on ptbdb\n",
    "# architect('ensemble', 'ptbdb', 'ensemble', 550, type_ids = [('rcnn', 250), ('cnn', 150)])\n",
    "# architect('ensemble', 'ptbdb', 'ensemble', 550, type_ids = [('rcnn', 250), ('cnn', 150)])\n",
    "\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use TFL\n",
    "print(\"TRANSFER LEARNING\")\n",
    "\n",
    "# Pretraining on MIT\n",
    "\n",
    "# Freeze RNN layers\n",
    "# transfer_learning(data='mitbih', type_id=('cnn', 100), id=700)\n",
    "# Train whole model\n",
    "transfer_learning(data='ptbdb', type_id=('cnn', 150), id=750)\n",
    "\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "${KERNEL_SPEC_DISPLAY_NAME}",
   "language": "${KERNEL_SPEC_LANGUAGE}",
   "name": "${KERNEL_SPEC_NAME}"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}